# OpenRouter Load Balancing ‡πÅ‡∏•‡∏∞ Automatic Fallbacks

**‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà:** 30 ‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2025  
**‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô:** SmartSpec Pro 0.2.0  
**‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô:** SmartSpec Team

---

## üìã **‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç**

1. [‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°](#‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°)
2. [Load Balancing (‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÇ‡∏´‡∏•‡∏î)](#load-balancing-‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÇ‡∏´‡∏•‡∏î)
3. [Automatic Fallbacks (‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)](#automatic-fallbacks-‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
4. [Provider Routing (‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Provider)](#provider-routing-‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å-provider)
5. [‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô Python](#‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô-python)
6. [‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô](#‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
7. [Best Practices](#best-practices)

---

## üéØ **‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°**

OpenRouter ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ **Load Balancing** ‡πÅ‡∏•‡∏∞ **Automatic Fallbacks** ‡πÅ‡∏ö‡∏ö built-in ‡πÄ‡∏û‡∏∑‡πà‡∏≠:

- ‚úÖ **‡πÄ‡∏û‡∏¥‡πà‡∏° uptime** - ‡∏ñ‡πâ‡∏≤ provider ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏•‡πâ‡∏° ‡∏à‡∏∞‡∏•‡∏≠‡∏á‡∏≠‡∏±‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- ‚úÖ **‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÇ‡∏´‡∏•‡∏î** - ‡πÅ‡∏ö‡πà‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏´‡∏•‡∏≤‡∏¢ providers ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î rate limiting
- ‚úÖ **‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô** - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô
- ‚úÖ **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß** - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (throughput/latency)
- ‚úÖ **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô** - ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

---

## ‚öñÔ∏è **Load Balancing (‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÇ‡∏´‡∏•‡∏î)**

### **1. Default Strategy: Price-Based Load Balancing**

OpenRouter ‡∏à‡∏∞‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏° **‡∏£‡∏≤‡∏Ñ‡∏≤** ‡πÅ‡∏•‡∏∞ **uptime**:

#### **‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°:**

```
1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å providers ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤
2. ‡∏à‡∏≤‡∏Å providers ‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å inverse square ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤
3. ‡πÉ‡∏ä‡πâ providers ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô fallbacks
```

#### **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:**

‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏°‡∏µ 3 providers:
- **Provider A:** $1 ‡∏ï‡πà‡∏≠ 1M tokens
- **Provider B:** $2 ‡∏ï‡πà‡∏≠ 1M tokens (‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢)
- **Provider C:** $3 ‡∏ï‡πà‡∏≠ 1M tokens

**‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÇ‡∏´‡∏•‡∏î:**
```
Provider A: ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å = 1/(1¬≤) = 1.0
Provider C: ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å = 1/(3¬≤) = 0.111

Provider A ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 9 ‡πÄ‡∏ó‡πà‡∏≤‡∏Ç‡∏≠‡∏á Provider C
```

**‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á:**
```
1. Provider A (‡∏ñ‡∏π‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î, ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£)
2. Provider C (‡∏ñ‡∏π‡∏Å‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤, ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£)
3. Provider B (‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤, ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô fallback ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)
```

### **2. Throughput-Based Load Balancing**

‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏ó‡∏µ‡πà‡∏°‡∏µ **throughput ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î** (tokens/‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ):

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="sk-or-v1-your-key"
)

response = client.chat.completions.create(
    model="meta-llama/llama-3.1-70b-instruct",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {
            "sort": "throughput"  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        }
    }
)
```

**‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ shortcut `:nitro`:**

```python
response = client.chat.completions.create(
    model="meta-llama/llama-3.1-70b-instruct:nitro",  # ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡πà‡∏≤ sort="throughput"
    messages=[{"role": "user", "content": "Hello"}]
)
```

### **3. Latency-Based Load Balancing**

‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏ó‡∏µ‡πà‡∏°‡∏µ **latency ‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î** (‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á):

```python
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {
            "sort": "latency"  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏ó‡∏µ‡πà latency ‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        }
    }
)
```

### **4. Price-Based Load Balancing (Explicit)**

‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏ó‡∏µ‡πà **‡∏ñ‡∏π‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î**:

```python
response = client.chat.completions.create(
    model="anthropic/claude-3.5-sonnet:floor",  # ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡πà‡∏≤ sort="price"
    messages=[{"role": "user", "content": "Hello"}]
)
```

---

## üîÑ **Automatic Fallbacks (‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)**

### **1. Default Behavior: Fallbacks Enabled**

OpenRouter ‡∏à‡∏∞‡∏•‡∏≠‡∏á fallback providers ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠:

- ‚úÖ Provider ‡∏´‡∏•‡∏±‡∏Å‡∏•‡πâ‡∏° (downtime)
- ‚úÖ Rate limiting (‡πÄ‡∏Å‡∏¥‡∏ô‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤)
- ‚úÖ Content moderation (‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤)
- ‚úÖ Context length error (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô)
- ‚úÖ Timeout
- ‚úÖ Error ‡∏≠‡∏∑‡πà‡∏ô‡πÜ

#### **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Model Fallbacks**

```python
response = client.chat.completions.create(
    model="anthropic/claude-3.5-sonnet",  # Model ‡∏´‡∏•‡∏±‡∏Å
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "models": [
            "openai/gpt-4o",           # Fallback #1
            "google/gemini-flash-1.5"  # Fallback #2
        ]
    }
)
```

**Flow:**
```
1. ‡∏•‡∏≠‡∏á claude-3.5-sonnet ‡∏Å‡πà‡∏≠‡∏ô
   ‚Üì (‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°)
2. ‡∏•‡∏≠‡∏á gpt-4o
   ‚Üì (‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°‡∏≠‡∏µ‡∏Å)
3. ‡∏•‡∏≠‡∏á gemini-flash-1.5
   ‚Üì (‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°‡∏≠‡∏µ‡∏Å)
4. Return error
```

**‡∏£‡∏≤‡∏Ñ‡∏≤:** ‡∏Ñ‡∏¥‡∏î‡∏ï‡∏≤‡∏° model ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á (‡∏î‡∏π‡∏à‡∏≤‡∏Å `response.model`)

### **2. Provider Fallbacks**

‡∏Å‡∏≥‡∏´‡∏ô‡∏î **‡∏•‡∏≥‡∏î‡∏±‡∏ö providers** ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á:

```python
response = client.chat.completions.create(
    model="mistralai/mixtral-8x7b-instruct",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {
            "order": ["together", "deepinfra", "fireworks"]  # ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        }
    }
)
```

**Flow:**
```
1. ‡∏•‡∏≠‡∏á Together AI ‡∏Å‡πà‡∏≠‡∏ô
   ‚Üì (‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°)
2. ‡∏•‡∏≠‡∏á DeepInfra
   ‚Üì (‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°‡∏≠‡∏µ‡∏Å)
3. ‡∏•‡∏≠‡∏á Fireworks
   ‚Üì (‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°‡∏≠‡∏µ‡∏Å)
4. ‡∏•‡∏≠‡∏á providers ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà OpenRouter ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏´‡πâ (default fallbacks)
```

### **3. Disabling Fallbacks**

‡∏õ‡∏¥‡∏î fallbacks ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ provider ‡πÄ‡∏â‡∏û‡∏≤‡∏∞:

```python
response = client.chat.completions.create(
    model="anthropic/claude-3.5-sonnet",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {
            "order": ["anthropic"],
            "allow_fallbacks": False  # ‡∏õ‡∏¥‡∏î fallbacks
        }
    }
)
```

**Flow:**
```
1. ‡∏•‡∏≠‡∏á Anthropic
   ‚Üì (‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°)
2. Return error ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÑ‡∏°‡πà‡∏•‡∏≠‡∏á providers ‡∏≠‡∏∑‡πà‡∏ô)
```

---

## üéØ **Provider Routing (‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Provider)**

### **1. Allow Only Specific Providers**

‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ providers ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏:

```python
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {
            "only": ["azure", "openai"]  # ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Azure ‡πÅ‡∏•‡∏∞ OpenAI
        }
    }
)
```

### **2. Ignore Specific Providers**

‡∏Ç‡πâ‡∏≤‡∏° providers ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:

```python
response = client.chat.completions.create(
    model="meta-llama/llama-3.3-70b-instruct",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {
            "ignore": ["deepinfra"]  # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ DeepInfra
        }
    }
)
```

### **3. Targeting Specific Provider Endpoints**

‡∏ö‡∏≤‡∏á providers ‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ endpoints (‡πÄ‡∏ä‡πà‡∏ô default, turbo):

```python
response = client.chat.completions.create(
    model="deepseek/deepseek-r1",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {
            "order": ["deepinfra/turbo"],  # ‡πÉ‡∏ä‡πâ turbo endpoint
            "allow_fallbacks": False
        }
    }
)
```

### **4. Require Parameter Support**

‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ providers ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö parameters ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:

```python
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    response_format={"type": "json_object"},  # ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ JSON output
    extra_body={
        "provider": {
            "require_parameters": True  # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ providers ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö JSON
        }
    }
)
```

### **5. Data Privacy Controls**

#### **5.1 Deny Data Collection**

‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ providers ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:

```python
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Sensitive data"}],
    extra_body={
        "provider": {
            "data_collection": "deny"  # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ providers ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        }
    }
)
```

#### **5.2 Zero Data Retention (ZDR)**

‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ ZDR endpoints:

```python
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Confidential data"}],
    extra_body={
        "provider": {
            "zdr": True  # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ZDR endpoints
        }
    }
)
```

### **6. Quantization Control**

‡πÄ‡∏•‡∏∑‡∏≠‡∏Å quantization level (int4, int8, fp8, fp16):

```python
response = client.chat.completions.create(
    model="meta-llama/llama-3.1-70b-instruct",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {
            "quantizations": ["fp8", "fp16"]  # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ FP8 ‡∏´‡∏£‡∏∑‡∏≠ FP16
        }
    }
)
```

### **7. Max Price Limit**

‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏≤‡∏Ñ‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏à‡πà‡∏≤‡∏¢:

```python
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {
            "max_price": {
                "prompt": 0.005,      # ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î $0.005 ‡∏ï‡πà‡∏≠ 1K prompt tokens
                "completion": 0.015   # ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î $0.015 ‡∏ï‡πà‡∏≠ 1K completion tokens
            }
        }
    }
)
```

---

## üêç **‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô Python**

### **1. Basic Setup**

```python
from openai import OpenAI

# Setup OpenRouter client
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="sk-or-v1-your-key"
)

# Optional: Add site info for rankings
import os
os.environ["OPENROUTER_SITE_URL"] = "https://yoursite.com"
os.environ["OPENROUTER_SITE_NAME"] = "Your App Name"
```

### **2. Helper Function ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Provider Options**

```python
from typing import Optional, List, Literal

def create_provider_options(
    sort: Optional[Literal["price", "throughput", "latency"]] = None,
    order: Optional[List[str]] = None,
    allow_fallbacks: bool = True,
    require_parameters: bool = False,
    data_collection: Literal["allow", "deny"] = "allow",
    zdr: Optional[bool] = None,
    only: Optional[List[str]] = None,
    ignore: Optional[List[str]] = None,
    quantizations: Optional[List[str]] = None,
    max_price: Optional[dict] = None
) -> dict:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á provider options ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenRouter
    
    Args:
        sort: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° "price", "throughput", ‡∏´‡∏£‡∏∑‡∏≠ "latency"
        order: ‡∏•‡∏≥‡∏î‡∏±‡∏ö providers ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á
        allow_fallbacks: ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï fallbacks ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        require_parameters: ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ providers ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö parameters ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        data_collection: "allow" ‡∏´‡∏£‡∏∑‡∏≠ "deny" ‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        zdr: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ Zero Data Retention
        only: ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ providers ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
        ignore: ‡∏Ç‡πâ‡∏≤‡∏° providers ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
        quantizations: quantization levels ‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö
        max_price: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏à‡πà‡∏≤‡∏¢
    
    Returns:
        dict: provider options ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö extra_body
    """
    options = {}
    
    if sort:
        options["sort"] = sort
    if order:
        options["order"] = order
    if not allow_fallbacks:
        options["allow_fallbacks"] = False
    if require_parameters:
        options["require_parameters"] = True
    if data_collection == "deny":
        options["data_collection"] = "deny"
    if zdr is not None:
        options["zdr"] = zdr
    if only:
        options["only"] = only
    if ignore:
        options["ignore"] = ignore
    if quantizations:
        options["quantizations"] = quantizations
    if max_price:
        options["max_price"] = max_price
    
    return {"provider": options} if options else {}


# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
provider_opts = create_provider_options(
    sort="throughput",
    allow_fallbacks=True,
    data_collection="deny"
)

response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body=provider_opts
)
```

### **3. Wrapper Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SmartSpec**

```python
class OpenRouterClient:
    """
    Wrapper class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenRouter ‡πÉ‡∏ô SmartSpec Pro
    """
    
    def __init__(self, api_key: str, site_url: str = "", site_name: str = ""):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key
        )
        self.site_url = site_url
        self.site_name = site_name
    
    def chat(
        self,
        model: str,
        messages: List[dict],
        # Load balancing options
        sort: Optional[Literal["price", "throughput", "latency"]] = None,
        # Fallback options
        fallback_models: Optional[List[str]] = None,
        allow_fallbacks: bool = True,
        # Provider options
        preferred_providers: Optional[List[str]] = None,
        only_providers: Optional[List[str]] = None,
        ignore_providers: Optional[List[str]] = None,
        # Privacy options
        data_collection: Literal["allow", "deny"] = "allow",
        zdr: Optional[bool] = None,
        # Other options
        require_parameters: bool = False,
        quantizations: Optional[List[str]] = None,
        max_price: Optional[dict] = None,
        **kwargs
    ):
        """
        ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ OpenRouter chat completion ‡∏û‡∏£‡πâ‡∏≠‡∏° load balancing ‡πÅ‡∏•‡∏∞ fallbacks
        
        Args:
            model: Model ID (e.g., "openai/gpt-4o")
            messages: Chat messages
            sort: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á providers ‡∏ï‡∏≤‡∏° "price", "throughput", "latency"
            fallback_models: Model fallbacks
            allow_fallbacks: ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï provider fallbacks
            preferred_providers: ‡∏•‡∏≥‡∏î‡∏±‡∏ö providers ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            only_providers: ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ providers ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
            ignore_providers: ‡∏Ç‡πâ‡∏≤‡∏° providers ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
            data_collection: "allow" ‡∏´‡∏£‡∏∑‡∏≠ "deny"
            zdr: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Zero Data Retention
            require_parameters: ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ providers ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö parameters ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            quantizations: Quantization levels
            max_price: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            **kwargs: OpenAI API parameters ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
        
        Returns:
            ChatCompletion response
        """
        # Build extra_body
        extra_body = {}
        
        # Provider options
        provider_opts = {}
        if sort:
            provider_opts["sort"] = sort
        if preferred_providers:
            provider_opts["order"] = preferred_providers
        if not allow_fallbacks:
            provider_opts["allow_fallbacks"] = False
        if require_parameters:
            provider_opts["require_parameters"] = True
        if data_collection == "deny":
            provider_opts["data_collection"] = "deny"
        if zdr is not None:
            provider_opts["zdr"] = zdr
        if only_providers:
            provider_opts["only"] = only_providers
        if ignore_providers:
            provider_opts["ignore"] = ignore_providers
        if quantizations:
            provider_opts["quantizations"] = quantizations
        if max_price:
            provider_opts["max_price"] = max_price
        
        if provider_opts:
            extra_body["provider"] = provider_opts
        
        # Model fallbacks
        if fallback_models:
            extra_body["models"] = fallback_models
        
        # Site info headers
        extra_headers = {}
        if self.site_url:
            extra_headers["HTTP-Referer"] = self.site_url
        if self.site_name:
            extra_headers["X-Title"] = self.site_name
        
        # Make request
        return self.client.chat.completions.create(
            model=model,
            messages=messages,
            extra_body=extra_body if extra_body else None,
            extra_headers=extra_headers if extra_headers else None,
            **kwargs
        )


# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
or_client = OpenRouterClient(
    api_key="sk-or-v1-your-key",
    site_url="https://smartspec.pro",
    site_name="SmartSpec Pro"
)

# Example 1: High throughput with fallbacks
response = or_client.chat(
    model="anthropic/claude-3.5-sonnet",
    messages=[{"role": "user", "content": "Write code"}],
    sort="throughput",
    fallback_models=["openai/gpt-4o", "google/gemini-flash-1.5"]
)

# Example 2: Privacy-focused
response = or_client.chat(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Sensitive data"}],
    data_collection="deny",
    zdr=True
)

# Example 3: Cost-optimized
response = or_client.chat(
    model="meta-llama/llama-3.1-70b-instruct",
    messages=[{"role": "user", "content": "Hello"}],
    sort="price",
    max_price={"prompt": 0.001, "completion": 0.002}
)
```

---

## üí° **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô**

### **Example 1: High Availability Setup**

‡πÄ‡∏ô‡πâ‡∏ô **uptime ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î** ‡∏û‡∏£‡πâ‡∏≠‡∏° fallbacks ‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏±‡πâ‡∏ô:

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="sk-or-v1-your-key"
)

response = client.chat.completions.create(
    model="anthropic/claude-3.5-sonnet",  # Model ‡∏´‡∏•‡∏±‡∏Å
    messages=[{"role": "user", "content": "Write a Python function"}],
    extra_body={
        "models": [
            "openai/gpt-4o",              # Fallback #1
            "google/gemini-flash-1.5",    # Fallback #2
            "meta-llama/llama-3.1-70b-instruct"  # Fallback #3
        ],
        "provider": {
            "allow_fallbacks": True  # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï provider fallbacks ‡∏î‡πâ‡∏ß‡∏¢
        }
    }
)

print(f"Used model: {response.model}")
print(f"Response: {response.choices[0].message.content}")
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
- ‡∏ñ‡πâ‡∏≤ Claude ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‚Üí ‡πÉ‡∏ä‡πâ Claude
- ‡∏ñ‡πâ‡∏≤ Claude ‡∏•‡πâ‡∏° ‚Üí ‡∏•‡∏≠‡∏á GPT-4o
- ‡∏ñ‡πâ‡∏≤ GPT-4o ‡∏•‡πâ‡∏° ‚Üí ‡∏•‡∏≠‡∏á Gemini Flash
- ‡∏ñ‡πâ‡∏≤ Gemini ‡∏•‡πâ‡∏° ‚Üí ‡∏•‡∏≠‡∏á Llama 3.1
- ‡∏ñ‡πâ‡∏≤‡∏ó‡∏∏‡∏Å model ‡∏•‡πâ‡∏° ‚Üí Return error

### **Example 2: Cost-Optimized Setup**

‡πÄ‡∏ô‡πâ‡∏ô **‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î**:

```python
response = client.chat.completions.create(
    model="meta-llama/llama-3.1-70b-instruct:floor",  # ‡πÉ‡∏ä‡πâ provider ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    messages=[{"role": "user", "content": "Simple task"}],
    extra_body={
        "provider": {
            "max_price": {
                "prompt": 0.0005,      # ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î $0.0005 ‡∏ï‡πà‡∏≠ 1K prompt tokens
                "completion": 0.001    # ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î $0.001 ‡∏ï‡πà‡∏≠ 1K completion tokens
            }
        }
    }
)
```

### **Example 3: Speed-Optimized Setup**

‡πÄ‡∏ô‡πâ‡∏ô **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î**:

```python
response = client.chat.completions.create(
    model="google/gemini-flash-1.5:nitro",  # ‡πÉ‡∏ä‡πâ provider ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    messages=[{"role": "user", "content": "Quick question"}],
    max_tokens=100  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î output ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
)
```

### **Example 4: Privacy-Focused Setup**

‡πÄ‡∏ô‡πâ‡∏ô **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß**:

```python
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Confidential business data"}],
    extra_body={
        "provider": {
            "zdr": True,                    # Zero Data Retention
            "data_collection": "deny",      # ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            "only": ["azure", "openai"]     # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ providers ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠
        }
    }
)
```

### **Example 5: Production-Ready Setup**

‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á:

```python
def call_llm_with_high_reliability(
    model: str,
    messages: List[dict],
    task_type: str = "general"
) -> dict:
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏û‡∏£‡πâ‡∏≠‡∏° load balancing ‡πÅ‡∏•‡∏∞ fallbacks ‡πÅ‡∏ö‡∏ö production-ready
    """
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î fallbacks ‡∏ï‡∏≤‡∏° task type
    fallback_configs = {
        "code": {
            "models": [
                "anthropic/claude-3.5-sonnet",
                "openai/gpt-4o",
                "google/gemini-flash-1.5"
            ],
            "sort": "quality"
        },
        "speed": {
            "models": [
                "google/gemini-flash-1.5",
                "openai/gpt-4o-mini",
                "meta-llama/llama-3.1-70b-instruct"
            ],
            "sort": "throughput"
        },
        "cost": {
            "models": [
                "meta-llama/llama-3.1-70b-instruct",
                "google/gemini-flash-1.5",
                "openai/gpt-4o-mini"
            ],
            "sort": "price"
        }
    }
    
    config = fallback_configs.get(task_type, fallback_configs["code"])
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            extra_body={
                "models": config["models"],
                "provider": {
                    "sort": config.get("sort"),
                    "allow_fallbacks": True,
                    "require_parameters": True,
                    "data_collection": "deny"  # Privacy by default
                }
            },
            timeout=30  # 30 seconds timeout
        )
        
        return {
            "success": True,
            "content": response.choices[0].message.content,
            "model_used": response.model,
            "tokens": response.usage.total_tokens if response.usage else 0
        }
    
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
result = call_llm_with_high_reliability(
    model="anthropic/claude-3.5-sonnet",
    messages=[{"role": "user", "content": "Write a function"}],
    task_type="code"
)

if result["success"]:
    print(f"Model used: {result['model_used']}")
    print(f"Response: {result['content']}")
else:
    print(f"Error: {result['error']}")
```

---

## ‚úÖ **Best Practices**

### **1. Load Balancing**

```python
# ‚úÖ DO: ‡πÉ‡∏ä‡πâ default load balancing ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö general use
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}]
)

# ‚úÖ DO: ‡πÉ‡∏ä‡πâ sort ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏â‡∏û‡∏≤‡∏∞
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={"provider": {"sort": "throughput"}}  # ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
)

# ‚ùå DON'T: ‡πÉ‡∏ä‡πâ sort ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏µ‡∏¢ load balancing)
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={"provider": {"sort": "price"}}  # ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô default ‡∏Å‡πá‡∏î‡∏µ‡πÅ‡∏•‡πâ‡∏ß
)
```

### **2. Fallbacks**

```python
# ‚úÖ DO: ‡πÉ‡∏ä‡πâ model fallbacks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö high availability
response = client.chat.completions.create(
    model="anthropic/claude-3.5-sonnet",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "models": ["openai/gpt-4o", "google/gemini-flash-1.5"]
    }
)

# ‚úÖ DO: ‡πÉ‡∏ä‡πâ provider order ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ provider ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {"order": ["azure", "openai"]}
    }
)

# ‚ùå DON'T: ‡∏õ‡∏¥‡∏î fallbacks ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {"allow_fallbacks": False}  # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ uptime ‡∏ï‡πà‡∏≥
    }
)
```

### **3. Privacy**

```python
# ‚úÖ DO: ‡πÉ‡∏ä‡πâ ZDR ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà sensitive
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Confidential data"}],
    extra_body={
        "provider": {"zdr": True}
    }
)

# ‚úÖ DO: ‡πÉ‡∏ä‡πâ data_collection="deny" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö privacy
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Personal info"}],
    extra_body={
        "provider": {"data_collection": "deny"}
    }
)
```

### **4. Cost Control**

```python
# ‚úÖ DO: ‡πÉ‡∏ä‡πâ max_price ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={
        "provider": {
            "max_price": {
                "prompt": 0.005,
                "completion": 0.015
            }
        }
    }
)

# ‚úÖ DO: ‡πÉ‡∏ä‡πâ :floor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
response = client.chat.completions.create(
    model="meta-llama/llama-3.1-70b-instruct:floor",
    messages=[{"role": "user", "content": "Hello"}]
)
```

### **5. Error Handling**

```python
# ‚úÖ DO: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ errors ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
from openai import OpenAIError
import time

def call_with_retry(client, model, messages, max_retries=3):
    """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏û‡∏£‡πâ‡∏≠‡∏° retry logic"""
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                extra_body={
                    "models": [
                        "openai/gpt-4o",
                        "google/gemini-flash-1.5"
                    ]
                }
            )
            return response
        
        except OpenAIError as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff
                print(f"Attempt {attempt + 1} failed: {e}. Retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                print(f"All {max_retries} attempts failed")
                raise
```

---

## üìä **‡∏™‡∏£‡∏∏‡∏õ**

### **Load Balancing Strategies**

| Strategy | Use Case | Command |
|----------|----------|---------|
| **Default** | General use, cost-effective | `model="openai/gpt-4o"` |
| **Price** | ‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î | `model="...:floor"` ‡∏´‡∏£‡∏∑‡∏≠ `sort="price"` |
| **Throughput** | ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î | `model="...:nitro"` ‡∏´‡∏£‡∏∑‡∏≠ `sort="throughput"` |
| **Latency** | ‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î | `sort="latency"` |

### **Fallback Strategies**

| Strategy | Reliability | Cost | Complexity |
|----------|-------------|------|------------|
| **No fallbacks** | ‚≠ê | $ | Simple |
| **Provider fallbacks** | ‚≠ê‚≠ê‚≠ê | $$ | Medium |
| **Model fallbacks** | ‚≠ê‚≠ê‚≠ê‚≠ê | $$$ | Medium |
| **Both** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | $$$$ | Complex |

### **Privacy Options**

| Option | Privacy Level | Availability |
|--------|---------------|--------------|
| **Default** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **data_collection="deny"** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **zdr=True** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **only=[trusted]** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |

---

## üîó **References**

- **OpenRouter Docs:** https://openrouter.ai/docs
- **Model Routing:** https://openrouter.ai/docs/guides/features/model-routing
- **Provider Selection:** https://openrouter.ai/docs/guides/routing/provider-selection
- **SmartSpec OpenRouter Provider:** `python-backend/app/llm_proxy/providers/openrouter_provider.py`

---

## ‚úÖ **Checklist**

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ OpenRouter ‡πÉ‡∏ô production:

- [ ] ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API key ‡πÅ‡∏•‡∏∞ site info
- [ ] ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å load balancing strategy ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
- [ ] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î fallback models/providers
- [ ] ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ privacy options (ZDR, data_collection)
- [ ] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î max_price ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô
- [ ] ‡πÄ‡∏û‡∏¥‡πà‡∏° error handling ‡πÅ‡∏•‡∏∞ retry logic
- [ ] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö fallbacks ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
- [ ] Monitor usage ‡πÅ‡∏•‡∏∞ costs
- [ ] Log model_used ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö debugging

---

**‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô OpenRouter ‡πÅ‡∏ö‡∏ö production-ready! üöÄ**
